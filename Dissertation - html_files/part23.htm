<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>VI: Training</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part22.htm">&lt; Previous</a><span> | </span><a href="../Dissertation%20-%20html.html">Contents</a><span> | </span><a href="part24.htm">Next &gt;</a></p><h3 style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;text-align: justify;">VI: Training</h3><p style="padding-top: 6pt;padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">The images have been resized, labelled, split into different sets, and the model created; now, training is the next step. The training was done using the keras_seg script from the image segmentation keras framework [41]. A command line interface is used within the Docker [39] container to interact with the script. The Keras_seg script requires the model used, a path to save checkpoints in training, a path for the train images and annotations, a path for the validation images and annotations, the number of epochs to train, the number of classes, the input height and width and the model and the batch size. Within this project, the number of classes was set to 6 for cardboard, glass, metal, paper, plastic and trash; the input width was 256 and input height was 320, and the batch size was 1024. Both epochs and the model values changed to create different models to compare the results.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part22.htm">&lt; Previous</a><span> | </span><a href="../Dissertation%20-%20html.html">Contents</a><span> | </span><a href="part24.htm">Next &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
