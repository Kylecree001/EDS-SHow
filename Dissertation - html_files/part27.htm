<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>II: Evaluation</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part26.htm">&lt; Previous</a><span> | </span><a href="../Dissertation%20-%20html.html">Contents</a><span> | </span><a href="part28.htm">Next &gt;</a></p><h3 style="padding-left: 11pt;text-indent: 0pt;text-align: justify;">II: Evaluation</h3><p style="padding-top: 6pt;padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">For all six models tested, the frequency-weighted IOU value for all results is between 23-26%. Since the frequency-weighted value is how well the label scored vs how frequent the label was in the input image, we can see almost all labels; the accuracy is low. However, further looking at the results, the range of 23-26% is small and therefore implies that training the model for more epochs did not significantly affect the accuracy of the labels. Furthermore, the low range indicated that there was close to no difference between the unet and segnet models in terms of accuracy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">However, when looking at the predictions produced, the results favour the resnet50_segnet model with 50 epochs of training (figure 16, page 35). The resnet50_segnet model with 50 epochs of training had the most clustering of pixels that models the input image better than all other results. The labels, however, appear to be incorrect, leading to low IOU scores.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">The cardboard label was a clear winner, scoring almost 46% accuracy. Well, all the other labels got values around or less than 10%, with the only outlier being in the resnet50_unet model with 10 epochs training (table 1, page 27 ), where the trash label got an IOU score of 26.206308%. This model prediction results were run several times to ensure that this outlier was not a mistake. In the resnet50_unet model with 10 epochs training predictions (figure 11, page 28), it can be seen that the pixels in the prediction appear to be more or less random, not an actual representation of the input images. Therefore, the most likely reason for this outlier is that the predicted pixels are more or less lucky guesses that produce an unusually high IOU score. This theory is helped when looking at the other predictions, especially the resnet50_segnet model with 50 epochs training (figure 16, page 35), which appears to form pixel clusters closer to the input image, however, still producing an IOU score of  9.248911% (table 6, page 34). glass performed the worse out of all the labels, with the IOU score coming out lower than 1% for most values, with an upper end of 1.005619%. Following the trend of low accuracy, the metal label did not perform much better, with most values around 2% and an upper-end score of 2.862692%. Plastic performed better than both metal and glass. However, the IOU score was around 8%, with the upper values being 9.610298%. Paper did perform slightly better than plastic, with IOU scores around 10% and the upper end being 12.864499%.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">The answer starts with the images used when looking for the reason for these scores on the model. The training and test images were significantly different due to being unable to obtain the wanted</p><p style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">images. Training images consisted of a singular piece of rubbish or recycling on a white background; the testing data consisted of many different labels within a single image. This discrepancy was attempted to overcome using image segmentation. However, the significant difference in images could not be overcome. When inputting the testing data into the model, the model had never handled an image with more than one class, which led to many of the pixels being all over the place in the predictions. The general idea of machine learning is to show the model as many scenarios as possible, allowing the model to pick the pattern. What was done here with the training and test data was the equivalent of teaching a model how to drive a car in an empty car part and then getting the model to drive the car through rush hour traffic in Auckland.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">Regional images also played a part in the results of the model. All the training images were from the USA, and as such, the recycling and the rubbish are different. In addition, brands and logos play a large part in image detection, with the USA having significantly different brands than New Zealand. This difference would have further damaged the results from the training to the testing set.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">After looking through the training images, other factors throw off the model prediction. For example, in figure 17, the red outline shows two different items that are the same brand; therefore, colours and logos are similar. However, as can be seen, one is an ice cream wrapper and is not recyclable; the other is a cardboard box.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;text-align: left;"><span><img width="300" height="400" alt="image" src="Image_024.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;text-align: justify;">Figure 17: Same brand, different label example</p><p style="padding-top: 9pt;padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">Notably, from all the test images example, the is no glass. New Zealand has a separate bin in which the glass is placed. This is another reason why the training images not being from New Zealand poses a regional problem. The glass would likely not be a category with the initially wanted images. This difference does help explain the very low IOU score for the glass label.</p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part26.htm">&lt; Previous</a><span> | </span><a href="../Dissertation%20-%20html.html">Contents</a><span> | </span><a href="part28.htm">Next &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
