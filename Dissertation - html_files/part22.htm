<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>V: Model</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part21.htm">&lt; Previous</a><span> | </span><a href="../Dissertation%20-%20html.html">Contents</a><span> | </span><a href="part23.htm">Next &gt;</a></p><h3 style="padding-left: 11pt;text-indent: 0pt;text-align: justify;">V: Model</h3><p style="padding-top: 6pt;padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">Within this project, two different models will be used, and the results will be compared between them. Both models will be using the resnet50 encoder. This encoder uses 50 different layers, and its primary use is to allow the network to train many layers without increasing the training error. Training error is the error of the model between the predictions and the actual results in the training process. With more extensive networks and many layers, a problem called the vanishing gradient problem appears. To summarise, the vanishing gradient problem is when the gradient of the loss function approaches 0, and therefore the gradient decreases on an exponential scale as layers are added. This small gradient means that weights and biases of the starting layers are not updated to the best levels with each training session. Initial layers play a prominent role as they are often used to recognise the core elements of the data. Therefore this problem can lead to considerable inaccuracies throughout the entire model. Below are the layers used for the encoder (Figures 5, 6, 7 and 8).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span><img width="586" height="121" alt="image" src="Image_005.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Figure 5: Identity Block</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;text-align: left;"><span><img width="606" height="229" alt="image" src="Image_006.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">Figure 6: Conv Block</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 24pt;text-indent: 0pt;text-align: left;"><span><img width="440" height="184" alt="image" src="Image_007.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 11pt;text-indent: 0pt;text-align: left;">Figure 7: One Side Pad</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;text-align: left;"><span><img width="611" height="432" alt="image" src="Image_008.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;text-align: justify;">Figure 8: Resnet50 Encoder</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">After the encoder, the next part of the model layers is implemented. The first model is the Unet model. Unet has been widely used in segmentation tasks, mainly in biomedical industries, as it allows pixel-based classification. This is accomplished using the model to classify each pixel into a class. The layers within this Unet model are shown in figure 9.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;text-align: left;"><span><img width="599" height="250" alt="image" src="Image_009.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;text-align: justify;">Figure 9: Unet Diagram</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">The second model that Unet will be compared to is the Segnet model. Like Unet, Segnet is a segmentation model that classifies each pixel into a class. However, Segnet differs from Unet by transferring the pooling indices to the expansion path, which uses less memory. Unet, in comparison, transfers the entire feature map to the expansion path.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;text-align: left;"><span><img width="609" height="296" alt="image" src="Image_010.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 11pt;text-indent: 0pt;text-align: justify;">Figure 10: Segnet Model</p><p style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;line-height: 154%;text-align: justify;">Within the models that have been used, there are several different layers, each with there own functions and reason to be used within the model. Below is a list of all these layers and what their uses are.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 45pt;text-indent: -16pt;line-height: 152%;text-align: justify;"><span class="p">  ZeroPadding2D – This layer adds zeros to the rows and columns of all sides of an image tensor. Adding this layer allows the model to perform different arithmetic on the data without removing any data.</span></p><p class="s4" style="padding-left: 45pt;text-indent: -16pt;line-height: 152%;text-align: justify;"><span class="p">  Conv2D – The layer creates a convolution kernel that is convolved with the layer input to produce a tensor output. Convolution layers are mainly used to sharpen images and enhance edges.</span></p><p class="s4" style="padding-left: 45pt;text-indent: -16pt;line-height: 153%;text-align: justify;"><span class="p">  Batch Normalisation – Applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. An essential aspect of Batch Normalisation is that it behaves differently during training and inference. Batch normalisation stabilises the learning process, and therefore the amount of training epochs is significantly reduced, making training faster.</span></p><p class="s4" style="padding-left: 45pt;text-indent: -16pt;line-height: 151%;text-align: justify;"><span class="p">  UpSampling2D – Repeats the row and columns of the data by size[0] and size[1], respectively. Upsampling has the effect of transforming coarse features into more dense and detailed output.</span></p><p class="s4" style="padding-left: 45pt;text-indent: -16pt;line-height: 151%;text-align: justify;"><span class="p">  Concatenate – Takes an input of a list of tensors, all the same shape except for the concatenation axis, and returns a single tensor that is the concatenation of all inputs. Concatenate is used to take the inputs and concatenate them through a specific dimension.</span></p><p class="s4" style="padding-left: 45pt;text-indent: -16pt;line-height: 149%;text-align: justify;"><span class="p">  Activation – Applies an activation function to the output. Look into the use of the activation functions, maybe talk about the activation function algorithm</span></p><p class="s4" style="padding-left: 28pt;text-indent: 0pt;text-align: justify;"><span class="p">  Lambda – Custom function that can be applied to a layer</span></p><p class="s4" style="padding-top: 6pt;padding-left: 45pt;text-indent: -16pt;line-height: 153%;text-align: justify;"><span class="p">  MaxPooling2D – Down samples the input along spatial dimensions (height and width) by taking the max value over an input window for each input channel. Strides shift the window along each dimension. Max pooling&#39;s primary use is extracting low-level features such as edges and points, reducing variance and computation.</span></p><p class="s4" style="padding-left: 45pt;text-indent: -16pt;line-height: 152%;text-align: justify;"><span class="p">  AveragePooling2D - Down samples the input along spatial dimensions (height and width) by taking the average value over an input window for each input channel. Strides shift the window along each dimension. Average pooling&#39;s primary use is extracting smooth features, reducing the model&#39;s variance and computation.</span></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part21.htm">&lt; Previous</a><span> | </span><a href="../Dissertation%20-%20html.html">Contents</a><span> | </span><a href="part23.htm">Next &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
